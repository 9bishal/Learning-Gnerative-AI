{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6nlcXKcLZZAK2tV3gICzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9bishal/Learning-Gnerative-AI/blob/main/Text_Generation_with_Transformers_(GPT_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Gradio to wrap a text to text interface around GPT-2"
      ],
      "metadata": {
        "id": "YgvWugwBdk-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###installing and importing libraries"
      ],
      "metadata": {
        "id": "-Sin78zMdwkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGXEtkJFdr10",
        "outputId": "a682a677-2dd0-44b8-feac-7fbe80ee6a5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In collab, gradio is use as the frontend where we can input text and get a response"
      ],
      "metadata": {
        "id": "Si5wQiYaeXEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "kznX1vtgd53b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Will be loading the model with some customization along with tokenizers"
      ],
      "metadata": {
        "id": "mRwBZ9Avfz96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- GPT2Tokenizer: used to convert text to tokens\n",
        "TFGPT2LMHeadModel: the TensorFlow version of the GPT-2 model with a language modeling (LM) head, meaning it‚Äôs suitable for text generation.\n",
        " -->\n"
      ],
      "metadata": {
        "id": "8swQ6_pogjW8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYCPZ6YrmtvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model =  TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGYiZx9LfMEW",
        "outputId": "d4cc2d4f-c36b-43d0-a416-ee175dd9cf53"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2Tokenizer: used to convert text to tokens (numbers) that GPT-2 understands.\n",
        "\n",
        "TFGPT2LMHeadModel: the TensorFlow version of the GPT-2 model with a language modeling (LM) head, meaning it‚Äôs suitable for text generation."
      ],
      "metadata": {
        "id": "QqW05NC8g578"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(input):\n",
        "  input_ids =tokenizer.encode(input, return_tensors='tf')\n",
        "  beam_output = model.generate(input_ids,\n",
        "                               max_length=100,\n",
        "                               num_beams=5,\n",
        "                               no_repeat_ngram_size=2, #Prevents repeating 2-word phrases in the output\n",
        "                               early_stopping=True #Stops early when best sequence is likely found\n",
        "                               )\n",
        "  output = tokenizer.decode(beam_output[0],\n",
        "                            skip_special_tokens=True, #Removes special tokens like ``.\n",
        "                            clean_up_tokenization_spaces=True#Cleans extra spaces added during decoding.\n",
        "                            )\n",
        "  return \".\".join(output.split(\".\")[:-1])+\".\""
      ],
      "metadata": {
        "id": "iKigDJBpgWeR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNseHK1TjH42"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating the inference and launching"
      ],
      "metadata": {
        "id": "hAwtbh0HkYda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Gradio modern API\n",
        "gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Start with a sentence...\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"‚ú® GPT-2 Text Generator\",\n",
        "    description=\"üöÄ Try out OpenAI's GPT-2 model! Just write a sentence and let it continue the story. It might take a few seconds. Enjoy the magic of AI! ‚ú®\"\n",
        ").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "ug8bRUlmkbnX",
        "outputId": "5471e6d4-957a-4b3b-e8a1-0814258c2b2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4067aa363ef95d10dd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4067aa363ef95d10dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVJgVIL_lWcJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}